{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import imageio\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all landmarks in uppercase\n",
    "LANDMARKS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_EYE_INNER\",\n",
    "    \"LEFT_EYE\",\n",
    "    \"LEFT_EYE_OUTER\",\n",
    "    \"RIGHT_EYE_INNER\",\n",
    "    \"RIGHT_EYE\",\n",
    "    \"RIGHT_EYE_OUTER\",\n",
    "    \"LEFT_EAR\",\n",
    "    \"RIGHT_EAR\",\n",
    "    \"MOUTH_LEFT\",\n",
    "    \"MOUTH_RIGHT\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_PINKY\",\n",
    "    \"RIGHT_PINKY\",\n",
    "    \"LEFT_INDEX\",\n",
    "    \"RIGHT_INDEX\",\n",
    "    \"LEFT_THUMB\",\n",
    "    \"RIGHT_THUMB\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\"\n",
    "]\n",
    "\n",
    "BODY_ANGLES = ['left_elbow_angle',\n",
    " 'right_elbow_angle',\n",
    " 'left_armpit_angle',\n",
    " 'right_armpit_angle',\n",
    " 'left_hip_angle',\n",
    " 'right_hip_angle',\n",
    " 'left_knee_angle',\n",
    " 'right_knee_angle',\n",
    " 'left_side_collarbone_angle',\n",
    " 'right_side_collarbone_angle']\n",
    "\n",
    "BODY_HEIGHTS = [\n",
    "    'shoulder_width_x', 'shoulder_width_y',\n",
    "    'hip_width_x', 'hip_width_y',\n",
    "    'left_elbow_wrist_distance_x', 'left_elbow_wrist_distance_y',\n",
    "    'right_elbow_wrist_distance_x', 'right_elbow_wrist_distance_y',\n",
    "    'left_elbow_shoulder_distance_x', 'left_elbow_shoulder_distance_y',\n",
    "    'right_elbow_shoulder_distance_x', 'right_elbow_shoulder_distance_y',\n",
    "    'left_shoulder_hip_alignment_x', 'left_shoulder_hip_alignment_y',\n",
    "    'right_shoulder_hip_alignment_x', 'right_shoulder_hip_alignment_y',\n",
    "    'left_knee_ankle_distance_x', 'left_knee_ankle_distance_y',\n",
    "    'right_knee_ankle_distance_x', 'right_knee_ankle_distance_y',\n",
    "    'left_knee_hip_distance_x', 'left_knee_hip_distance_y',\n",
    "    'right_knee_hip_distance_x', 'right_knee_hip_distance_y',\n",
    "    'left_hand_shoulder_distance_x', 'left_hand_shoulder_distance_y',\n",
    "    'right_hand_shoulder_distance_x', 'right_hand_shoulder_distance_y',\n",
    "    'left_hand_nose_distance_x', 'left_hand_nose_distance_y',\n",
    "    'right_hand_nose_distance_x', 'right_hand_nose_distance_y',\n",
    "    'left_hand_hip_distance_x', 'left_hand_hip_distance_y',\n",
    "    'right_hand_hip_distance_x', 'right_hand_hip_distance_y',\n",
    "    'foot_spread_x', 'foot_spread_y'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get coordinates of a landmark\n",
    "def get_coords(landmark, df_frame):\n",
    "    coords = df_frame[df_frame['landmark'] == landmark][['x', 'y', 'z']].values\n",
    "    return coords[0] if len(coords) > 0 else None\n",
    "\n",
    "# Calculate vector between two points\n",
    "def calc_vector(p1, p2):\n",
    "    return np.array(p2) - np.array(p1)\n",
    "\n",
    "# Calculate angle between two vectors using the dot product\n",
    "def angle_between(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    cos_theta = dot_product / (norm_v1 * norm_v2)\n",
    "    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip to avoid numerical errors\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def get_body_vecs(coordinates):\n",
    "    # Upper arms\n",
    "    left_upper_arm = calc_vector(coordinates['LEFT_SHOULDER'], coordinates['LEFT_ELBOW'])\n",
    "    right_upper_arm = calc_vector(coordinates['RIGHT_SHOULDER'], coordinates['RIGHT_ELBOW'])\n",
    "\n",
    "    # Forearms\n",
    "    left_forearm = calc_vector(coordinates['LEFT_ELBOW'], coordinates['LEFT_WRIST'])\n",
    "    right_forearm = calc_vector(coordinates['RIGHT_ELBOW'], coordinates['RIGHT_WRIST'])\n",
    "\n",
    "    # Upper legs\n",
    "    left_upper_leg = calc_vector(coordinates['LEFT_HIP'], coordinates['LEFT_KNEE'])\n",
    "    right_upper_leg = calc_vector(coordinates['RIGHT_HIP'], coordinates['RIGHT_KNEE'])\n",
    "\n",
    "    # Lower legs\n",
    "    left_lower_leg = calc_vector(coordinates['LEFT_KNEE'], coordinates['LEFT_ANKLE'])\n",
    "    right_lower_leg = calc_vector(coordinates['RIGHT_KNEE'], coordinates['RIGHT_ANKLE'])\n",
    "\n",
    "    # Groin (hip-to-hip vector)\n",
    "    groin = calc_vector(coordinates['LEFT_HIP'], coordinates['RIGHT_HIP'])\n",
    "\n",
    "    # Collarbones (shoulder-to-shoulder vector)\n",
    "    collarbones = calc_vector(coordinates['LEFT_SHOULDER'], coordinates['RIGHT_SHOULDER'])\n",
    "\n",
    "    # Sides \n",
    "    right_side = calc_vector(coordinates['RIGHT_SHOULDER'], coordinates['RIGHT_HIP'])\n",
    "    left_side = calc_vector(coordinates['LEFT_SHOULDER'], coordinates['LEFT_HIP'])\n",
    "\n",
    "    # Return all vectors in a dictionary\n",
    "    return {\n",
    "        \"left_upper_arm\": left_upper_arm,\n",
    "        \"right_upper_arm\": right_upper_arm,\n",
    "        \"left_forearm\": left_forearm,\n",
    "        \"right_forearm\": right_forearm,\n",
    "        \"left_upper_leg\": left_upper_leg,\n",
    "        \"right_upper_leg\": right_upper_leg,\n",
    "        \"left_lower_leg\": left_lower_leg,\n",
    "        \"right_lower_leg\": right_lower_leg,\n",
    "        \"groin\": groin,\n",
    "        \"collarbones\": collarbones,\n",
    "        \"right_side\": right_side,\n",
    "        \"left_side\": left_side\n",
    "    }\n",
    "\n",
    "# Calculate Angles\n",
    "def calculate_body_angles(vectors):\n",
    "    # Elbow angles (between upper arm and forearm)\n",
    "    left_elbow_angle = angle_between(vectors['left_upper_arm'], vectors['left_forearm'])\n",
    "    right_elbow_angle = angle_between(vectors['right_upper_arm'], vectors['right_forearm'])\n",
    "\n",
    "    # Armpit angles (between upper arm and collarbones)\n",
    "    left_armpit_angle = angle_between(vectors['left_upper_arm'], vectors['collarbones'])\n",
    "    right_armpit_angle = angle_between(vectors['right_upper_arm'], vectors['collarbones'])\n",
    "\n",
    "    # Hip angles (between upper leg and side vectors, representing torso alignment)\n",
    "    left_hip_angle = angle_between(vectors['left_upper_leg'], vectors['left_side'])\n",
    "    right_hip_angle = angle_between(vectors['right_upper_leg'], vectors['right_side'])\n",
    "\n",
    "    # Knee angles (between upper leg and lower leg)\n",
    "    left_knee_angle = angle_between(vectors['left_upper_leg'], vectors['left_lower_leg'])\n",
    "    right_knee_angle = angle_between(vectors['right_upper_leg'], vectors['right_lower_leg'])\n",
    "\n",
    "    # Side to collarbone angles (lateral torso tilt)\n",
    "    left_side_collarbone_angle = angle_between(vectors['left_side'], vectors['collarbones'])\n",
    "    right_side_collarbone_angle = angle_between(vectors['right_side'], vectors['collarbones'])\n",
    "\n",
    "    # Return all angles in a dictionary\n",
    "    return {\n",
    "        \"left_elbow_angle\": left_elbow_angle,\n",
    "        \"right_elbow_angle\": right_elbow_angle,\n",
    "        \"left_armpit_angle\": left_armpit_angle,\n",
    "        \"right_armpit_angle\": right_armpit_angle,\n",
    "        \"left_hip_angle\": left_hip_angle,\n",
    "        \"right_hip_angle\": right_hip_angle,\n",
    "        \"left_knee_angle\": left_knee_angle,\n",
    "        \"right_knee_angle\": right_knee_angle,\n",
    "        \"left_side_collarbone_angle\": left_side_collarbone_angle,\n",
    "        \"right_side_collarbone_angle\": right_side_collarbone_angle\n",
    "    }\n",
    "\n",
    "# Helper function to calculate differences in x and y directions between two points\n",
    "def calc_xy_difference(p1, p2):\n",
    "    return p2[0] - p1[0], p2[1] - p1[1] \n",
    "\n",
    "# Calculate Heights\n",
    "def calculate_body_heights(coordinates):\n",
    "    # Calculate relevant x and y differences\n",
    "    heights = {\n",
    "        # Shoulder width (left shoulder to right shoulder)\n",
    "        \"shoulder_width_x\": calc_xy_difference(coordinates['LEFT_SHOULDER'], coordinates['RIGHT_SHOULDER'])[0],\n",
    "        \"shoulder_width_y\": calc_xy_difference(coordinates['LEFT_SHOULDER'], coordinates['RIGHT_SHOULDER'])[1],\n",
    "\n",
    "        # Hip width (left hip to right hip)\n",
    "        \"hip_width_x\": calc_xy_difference(coordinates['LEFT_HIP'], coordinates['RIGHT_HIP'])[0],\n",
    "        \"hip_width_y\": calc_xy_difference(coordinates['LEFT_HIP'], coordinates['RIGHT_HIP'])[1],\n",
    "\n",
    "        # Elbow-Wrist distance for arms\n",
    "        \"left_elbow_wrist_distance_x\": calc_xy_difference(coordinates['LEFT_ELBOW'], coordinates['LEFT_WRIST'])[0],\n",
    "        \"left_elbow_wrist_distance_y\": calc_xy_difference(coordinates['LEFT_ELBOW'], coordinates['LEFT_WRIST'])[1],\n",
    "        \"right_elbow_wrist_distance_x\": calc_xy_difference(coordinates['RIGHT_ELBOW'], coordinates['RIGHT_WRIST'])[0],\n",
    "        \"right_elbow_wrist_distance_y\": calc_xy_difference(coordinates['RIGHT_ELBOW'], coordinates['RIGHT_WRIST'])[1],\n",
    "\n",
    "        # Elbow-Shoulder distance for arms\n",
    "        \"left_elbow_shoulder_distance_x\": calc_xy_difference(coordinates['LEFT_ELBOW'], coordinates['LEFT_SHOULDER'])[0],\n",
    "        \"left_elbow_shoulder_distance_y\": calc_xy_difference(coordinates['LEFT_ELBOW'], coordinates['LEFT_SHOULDER'])[1],\n",
    "        \"right_elbow_shoulder_distance_x\": calc_xy_difference(coordinates['RIGHT_ELBOW'], coordinates['RIGHT_SHOULDER'])[0],\n",
    "        \"right_elbow_shoulder_distance_y\": calc_xy_difference(coordinates['RIGHT_ELBOW'], coordinates['RIGHT_SHOULDER'])[1],\n",
    "\n",
    "        # Shoulder-Hip alignment for left and right sides\n",
    "        \"left_shoulder_hip_alignment_x\": calc_xy_difference(coordinates['LEFT_SHOULDER'], coordinates['LEFT_HIP'])[0],\n",
    "        \"left_shoulder_hip_alignment_y\": calc_xy_difference(coordinates['LEFT_SHOULDER'], coordinates['LEFT_HIP'])[1],\n",
    "        \"right_shoulder_hip_alignment_x\": calc_xy_difference(coordinates['RIGHT_SHOULDER'], coordinates['RIGHT_HIP'])[0],\n",
    "        \"right_shoulder_hip_alignment_y\": calc_xy_difference(coordinates['RIGHT_SHOULDER'], coordinates['RIGHT_HIP'])[1],\n",
    "\n",
    "        # Knee-Ankle distance for legs\n",
    "        \"left_knee_ankle_distance_x\": calc_xy_difference(coordinates['LEFT_KNEE'], coordinates['LEFT_ANKLE'])[0],\n",
    "        \"left_knee_ankle_distance_y\": calc_xy_difference(coordinates['LEFT_KNEE'], coordinates['LEFT_ANKLE'])[1],\n",
    "        \"right_knee_ankle_distance_x\": calc_xy_difference(coordinates['RIGHT_KNEE'], coordinates['RIGHT_ANKLE'])[0],\n",
    "        \"right_knee_ankle_distance_y\": calc_xy_difference(coordinates['RIGHT_KNEE'], coordinates['RIGHT_ANKLE'])[1],\n",
    "\n",
    "        # Knee-Hip distance for legs\n",
    "        \"left_knee_hip_distance_x\": calc_xy_difference(coordinates['LEFT_KNEE'], coordinates['LEFT_HIP'])[0],\n",
    "        \"left_knee_hip_distance_y\": calc_xy_difference(coordinates['LEFT_KNEE'], coordinates['LEFT_HIP'])[1],\n",
    "        \"right_knee_hip_distance_x\": calc_xy_difference(coordinates['RIGHT_KNEE'], coordinates['RIGHT_HIP'])[0],\n",
    "        \"right_knee_hip_distance_y\": calc_xy_difference(coordinates['RIGHT_KNEE'], coordinates['RIGHT_HIP'])[1],\n",
    "\n",
    "        # Wrist-Shoulder distance for arms\n",
    "        \"left_hand_shoulder_distance_x\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['LEFT_SHOULDER'])[0],\n",
    "        \"left_hand_shoulder_distance_y\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['LEFT_SHOULDER'])[1],\n",
    "        \"right_hand_shoulder_distance_x\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['RIGHT_SHOULDER'])[0],\n",
    "        \"right_hand_shoulder_distance_y\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['RIGHT_SHOULDER'])[1],\n",
    "\n",
    "        # Wrist-Nose distance\n",
    "        \"left_hand_nose_distance_x\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['NOSE'])[0],\n",
    "        \"left_hand_nose_distance_y\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['NOSE'])[1],\n",
    "        \"right_hand_nose_distance_x\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['NOSE'])[0],\n",
    "        \"right_hand_nose_distance_y\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['NOSE'])[1],\n",
    "\n",
    "        # Wrist-Hip distance\n",
    "        \"left_hand_hip_distance_x\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['LEFT_HIP'])[0],\n",
    "        \"left_hand_hip_distance_y\": calc_xy_difference(coordinates['LEFT_WRIST'], coordinates['LEFT_HIP'])[1],\n",
    "        \"right_hand_hip_distance_x\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['RIGHT_HIP'])[0],\n",
    "        \"right_hand_hip_distance_y\": calc_xy_difference(coordinates['RIGHT_WRIST'], coordinates['RIGHT_HIP'])[1],\n",
    "\n",
    "        # Foot spread (left foot index to right foot index)\n",
    "        \"foot_spread_x\": calc_xy_difference(coordinates['LEFT_FOOT_INDEX'], coordinates['RIGHT_FOOT_INDEX'])[0],\n",
    "        \"foot_spread_y\": calc_xy_difference(coordinates['LEFT_FOOT_INDEX'], coordinates['RIGHT_FOOT_INDEX'])[1]\n",
    "    }\n",
    "\n",
    "    return heights\n",
    "\n",
    "\n",
    "def calculate_angle(A, B, C):\n",
    "    \"\"\"\n",
    "    Calculate the angle (in degrees) between vectors AB and BC.\n",
    "    A, B, C are tuples representing (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    # Create vectors AB and BC\n",
    "    AB = np.array([B[0] - A[0], B[1] - A[1]])\n",
    "    BC = np.array([C[0] - B[0], C[1] - B[1]])\n",
    "\n",
    "    # Calculate the dot product and magnitudes (norms) of the vectors\n",
    "    dot_product = np.dot(AB, BC)\n",
    "    magnitude_AB = np.linalg.norm(AB)\n",
    "    magnitude_BC = np.linalg.norm(BC)\n",
    "\n",
    "    # Calculate the cosine of the angle\n",
    "    cos_theta = dot_product / (magnitude_AB * magnitude_BC)\n",
    "\n",
    "    # Handle numerical precision issues (cosine should be between -1 and 1)\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "\n",
    "    # Calculate the angle in radians and convert to degrees\n",
    "    angle_rad = np.arccos(cos_theta)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    return angle_deg\n",
    "\n",
    "def create_features_per_excercise(video_folder, excercise_type,mp_pose):\n",
    "    excercise_path = os.path.join(video_folder, excercise_type)  # Construct the full path\n",
    "    vid_count = 0\n",
    "    csv_data = []\n",
    "    if os.path.isdir(excercise_path):  # Ensure it's a directory\n",
    "        for video_file in tqdm(os.listdir(excercise_path)):\n",
    "            video_path = os.path.join(excercise_path, video_file)  \n",
    "            # print(f\"Processing video: {video_path}\")\n",
    "\n",
    "            # Open video file\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video {video_path}.\")\n",
    "                continue\n",
    "            vid_count += 1\n",
    "            frame_number = 0\n",
    "\n",
    "            # Initialize MediaPipe Pose with default settings\n",
    "            with mp_pose.Pose() as pose:\n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    \n",
    "                    if not ret:\n",
    "                        # print(\"End of video or failed to read frame.\")\n",
    "                        break\n",
    "\n",
    "                    # Convert the frame to RGB (MediaPipe works with RGB images)\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Process the frame to detect pose landmarks\n",
    "                    result = pose.process(frame_rgb)\n",
    "\n",
    "                    # Draw pose landmarks if detected\n",
    "                    if result.pose_landmarks:\n",
    "                        for idx, landmark in enumerate(result.pose_landmarks.landmark):\n",
    "                            # print(f\"{mp_pose.PoseLandmark(idx).name}: (x: {landmark.x}, y: {landmark.y}, z: {landmark.z})\")\n",
    "                            csv_data.append([excercise_type,vid_count,frame_number, mp_pose.PoseLandmark(idx).name, landmark.x, landmark.y, landmark.z])\n",
    "                    # Increment frame number\n",
    "                    frame_number += 1\n",
    "            # Release video capture and close all windows\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    return csv_data\n",
    "\n",
    "def feat_to_parquet(csv_data, excercise_name):\n",
    "    df_feat = pd.DataFrame(csv_data)\n",
    "    df_feat = df_feat.rename(columns={\n",
    "        0: 'exc_name',\n",
    "        1: 'vid_num',\n",
    "        2: 'frame',\n",
    "        3: 'landmark',\n",
    "        4: 'x',\n",
    "        5: 'y',\n",
    "        6: 'z'\n",
    "    })\n",
    "    df_feat.to_parquet(f'{excercise_name}_features.parquet')\n",
    "\n",
    "def pad_sequence(sequence_chunk, desired_sequence_length):\n",
    "    num_rows_sequence = sequence_chunk.shape[0]\n",
    "    num_columns_sequence = sequence_chunk.shape[1]\n",
    "\n",
    "    # Determine the label value from the existing sequence\n",
    "    label_value = sequence_chunk['exc_name_encoded'].iloc[0]  # Assumes the label is the same for the whole chunk\n",
    "\n",
    "    # Create an empty sequence with the remaining rows, filled with 0\n",
    "    empty_sequence = pd.DataFrame(\n",
    "        np.full((desired_sequence_length - num_rows_sequence, num_columns_sequence), 0), \n",
    "        columns=sequence_chunk.columns\n",
    "    )\n",
    "    \n",
    "    # Set the label column in `empty_sequence` to the correct label\n",
    "    empty_sequence['exc_name_encoded'] = label_value\n",
    "\n",
    "    # Concatenate the original sequence with the padded rows\n",
    "    padded_sequence = pd.concat([sequence_chunk, empty_sequence], axis=0).reset_index(drop=True)\n",
    "\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Mediapipe Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run landmark detection on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose and Drawing modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Video capture path (ensure it points to your video correctly)\n",
    "video_path = r'excercise_videos\\barbell biceps curl\\0b43a151-8995-4f7e-8568-45d65996a19c.mp4'\n",
    "excercise_type_temp = 'bicep curl'\n",
    "vid_count_temp = 1\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f'Total number of frames: {frame_count}')\n",
    "\n",
    "# Get the frames per second (FPS)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f'Frames per second (FPS): {fps}')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video {video_path}.\")\n",
    "    exit()\n",
    "\n",
    "frame_number = 0\n",
    "csv_data = []\n",
    "landmark_lst = []\n",
    "\n",
    "# Initialize MediaPipe Pose with default settings\n",
    "with mp_pose.Pose() as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe works with RGB images)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect pose landmarks\n",
    "        result = pose.process(frame_rgb)\n",
    "\n",
    "        # Draw pose landmarks if detected\n",
    "        if result.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "\n",
    "            frame_landmarks = result.pose_landmarks.landmark\n",
    "            right_shoulder = (frame_landmarks[12].x,frame_landmarks[12].y)\n",
    "            right_elbow = (frame_landmarks[14].x,frame_landmarks[14].y)\n",
    "            right_wrist = (frame_landmarks[16].x,frame_landmarks[16].y)\n",
    "            frame_angle = -(calculate_angle(right_wrist, right_elbow, right_shoulder) - 180)\n",
    "\n",
    "            # Optional: Add landmark coordinates to CSV data\n",
    "            for idx, landmark in enumerate(result.pose_landmarks.landmark):\n",
    "                # print(f\"{mp_pose.PoseLandmark(idx).name}: (x: {landmark.x}, y: {landmark.y}, z: {landmark.z})\")\n",
    "                csv_data.append([excercise_type_temp,vid_count_temp,frame_number, mp_pose.PoseLandmark(idx).name, landmark.x, landmark.y, landmark.z])\n",
    "            landmark_lst.append(result.pose_landmarks)\n",
    "\n",
    "            # Add text to the frame\n",
    "        text = f\"Angle {round(frame_angle,2)}\"\n",
    "        org = (50, 50)  # Coordinates for the text (x, y)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX  # Font type\n",
    "        fontScale = 1  # Font size\n",
    "        color = (0, 255, 0)  # Green text in BGR\n",
    "        thickness = 2  # Thickness of the text\n",
    "        lineType = cv2.LINE_AA  # Anti-aliased text\n",
    "\n",
    "        # Write the text on the frame\n",
    "        cv2.putText(frame, text, org, font, fontScale, color, thickness, lineType)\n",
    "\n",
    "\n",
    "        # cv2.putText(frame, frame_angle)\n",
    "        # Display the frame in a window\n",
    "        cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "\n",
    "        # Wait for 1ms and exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release video capture and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trail_run = pd.DataFrame(csv_data)\n",
    "# Rename the columns from 0-5 to meaningful names\n",
    "df_trail_run = df_trail_run.rename(columns={\n",
    "    0: 'exc_name',\n",
    "    1: 'vid_num',\n",
    "    2: 'frame',\n",
    "    3: 'landmark',\n",
    "    4: 'x',\n",
    "    5: 'y',\n",
    "    6: 'z'\n",
    "})\n",
    "df_trail_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what one frame looks like\n",
    "df_trail_run[df_trail_run['frame'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_angles = []\n",
    "for frame_num in np.unique(df_trail_run['frame']):\n",
    "    right_elbow = (df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_ELBOW')].x.values[0], df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_ELBOW')].y.values[0])\n",
    "    right_shoulder = (df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_SHOULDER')].x.values[0],df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_SHOULDER')].y.values[0])\n",
    "    right_wrist = (df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_WRIST')].x.values[0],df_trail_run[(df_trail_run['frame'] == frame_num) & (df_trail_run['landmark'] == 'RIGHT_WRIST')].y.values[0])\n",
    "    frame_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "    frame_angles.append(frame_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(frame_angles)),frame_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the landmark list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_shoulder = (landmark_lst[0].landmark[12].x,landmark_lst[0].landmark[12].y)\n",
    "right_elbow = (landmark_lst[0].landmark[14].x,landmark_lst[0].landmark[14].y)\n",
    "right_wrist = (landmark_lst[0].landmark[16].x,landmark_lst[0].landmark[16].y)\n",
    "frame_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "frame_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial 3d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "frame_number = 0\n",
    "\n",
    "# Filter the data for the desired frame\n",
    "df_frame = df_trail_run[df_trail_run['frame'] == frame_number]\n",
    "\n",
    "# Create a dictionary mapping landmark names to their (x, y, z) coordinates\n",
    "landmarks = {row['landmark']: (row['x'], row['y'], row['z']) for _, row in df_frame.iterrows()}\n",
    "\n",
    "# Initialize the 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot landmarks as scatter points\n",
    "for name, (x, y, z) in landmarks.items():\n",
    "    ax.scatter(-z, x, y, label=name, s=50)  # Adjusting axes to align with desired orientation\n",
    "\n",
    "# Helper function to get coordinates by landmark name\n",
    "def get_coord(name):\n",
    "    \"\"\"Returns the (x, y, z) coordinates of a given landmark.\"\"\"\n",
    "    return landmarks.get(name, (None, None, None))\n",
    "\n",
    "# Plot lines connecting the landmarks using POSE_CONNECTIONS\n",
    "for connection in POSE_CONNECTIONS:\n",
    "    start, end = connection\n",
    "    start_name = mp_pose.PoseLandmark(start).name\n",
    "    end_name = mp_pose.PoseLandmark(end).name\n",
    "\n",
    "    # Get the coordinates of the two landmarks\n",
    "    start_coord = get_coord(start_name)\n",
    "    end_coord = get_coord(end_name)\n",
    "\n",
    "    # Check if both landmarks are available\n",
    "    if None not in start_coord and None not in end_coord:\n",
    "        ax.plot(\n",
    "            [-start_coord[2], -end_coord[2]],  # x-coordinates\n",
    "            [start_coord[0], end_coord[0]],    # y-coordinates\n",
    "            [start_coord[1], end_coord[1]],    # z-coordinates\n",
    "            color='blue'\n",
    "        )\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_zlabel('y')\n",
    "ax.set_xlabel('z')\n",
    "ax.set_ylabel('x')\n",
    "\n",
    "# Flip the Z-axis to ensure head is at the top and legs at the bottom\n",
    "ax.invert_zaxis()\n",
    "\n",
    "# Rotate, tilt, and twist the graph\n",
    "ax.view_init(elev=10, azim=20)  # Adjust 'elev' and 'azim' as needed\n",
    "\n",
    "# Set plot title\n",
    "plt.title(f'3D Pose for Frame {frame_number}')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive 3d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "frame_number = 0\n",
    "\n",
    "# Filter the data for the desired frame\n",
    "df_frame = df_trail_run[df_trail_run['frame'] == frame_number]\n",
    "\n",
    "# Create a dictionary mapping landmark names to their (x, y, z) coordinates\n",
    "landmarks = {row['landmark']: (row['x'], row['y'], row['z']) for _, row in df_frame.iterrows()}\n",
    "\n",
    "# Prepare lists to store points and connections\n",
    "x_vals, y_vals, z_vals, names = [], [], [], []\n",
    "\n",
    "# Collect the landmarks for plotting\n",
    "for name, (x, y, z) in landmarks.items():\n",
    "    x_vals.append(x)\n",
    "    y_vals.append(y)\n",
    "    z_vals.append(z)\n",
    "    names.append(name)\n",
    "\n",
    "# Create a 3D scatter plot for landmarks\n",
    "scatter = go.Scatter3d(\n",
    "    x=x_vals, y=z_vals, z=y_vals,  # Swap axes to align with your desired orientation\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=5, color='blue'),\n",
    "    text=names,  # Show names as text labels\n",
    "    textposition=\"top center\"\n",
    ")\n",
    "\n",
    "# Create line segments for each connection\n",
    "connections = []\n",
    "for connection in POSE_CONNECTIONS:\n",
    "    start, end = connection\n",
    "    start_name = mp_pose.PoseLandmark(start).name\n",
    "    end_name = mp_pose.PoseLandmark(end).name\n",
    "\n",
    "    start_coord = landmarks.get(start_name, None)\n",
    "    end_coord = landmarks.get(end_name, None)\n",
    "\n",
    "    # Ensure both landmarks exist\n",
    "    if start_coord and end_coord:\n",
    "        connections.append(\n",
    "            go.Scatter3d(\n",
    "                x=[start_coord[0], end_coord[0]],\n",
    "                y=[start_coord[2], end_coord[2]],  # Swap Y and Z axes\n",
    "                z=[start_coord[1], end_coord[1]],\n",
    "                mode='lines',\n",
    "                line=dict(color='blue', width=2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Combine scatter plot and connections into a figure\n",
    "fig = go.Figure(data=[scatter] + connections)\n",
    "\n",
    "# Set axis labels and layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Z',\n",
    "        zaxis_title='Y',\n",
    "        zaxis=dict(autorange='reversed')  # Ensure head is at the top and legs at the bottom\n",
    "    ),\n",
    "    title=f\"3D Pose for Frame {frame_number}\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn 3d plot into movie with multiple frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "\n",
    "# Create a directory to store individual frame images\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "\n",
    "# Determine the overall min and max values across all frames to set consistent axis limits\n",
    "x_min, x_max = df_trail_run['x'].min(), df_trail_run['x'].max()\n",
    "y_min, y_max = df_trail_run['y'].min(), df_trail_run['y'].max()\n",
    "z_min, z_max = df_trail_run['z'].min(), df_trail_run['z'].max()\n",
    "\n",
    "def get_coord(name, landmarks):\n",
    "    \"\"\"Returns the (x, y, z) coordinates of a given landmark.\"\"\"\n",
    "    return landmarks.get(name, (None, None, None))\n",
    "\n",
    "# Generate and save plots for each frame\n",
    "for frame_number in range(df_trail_run['frame'].max() + 1):\n",
    "    # Filter the data for the desired frame\n",
    "    df_trail_run_frame = df_trail_run[df_trail_run['frame'] == frame_number]\n",
    "\n",
    "    # Create a dictionary mapping landmark names to their (x, y, z) coordinates\n",
    "    landmarks = {row['landmark']: (row['x'], row['y'], row['z']) for _, row in df_trail_run_frame.iterrows()}\n",
    "\n",
    "    # Initialize the 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot landmarks as scatter points\n",
    "    for name, (x, y, z) in landmarks.items():\n",
    "        ax.scatter(-z, x, y, label=name, s=50)  # Adjusting axes to align with desired orientation\n",
    "\n",
    "    # Plot lines connecting the landmarks using POSE_CONNECTIONS\n",
    "    for connection in POSE_CONNECTIONS:\n",
    "        start, end = connection\n",
    "        start_name = mp_pose.PoseLandmark(start).name\n",
    "        end_name = mp_pose.PoseLandmark(end).name\n",
    "\n",
    "        start_coord = get_coord(start_name, landmarks)\n",
    "        end_coord = get_coord(end_name, landmarks)\n",
    "\n",
    "        # Check if both landmarks are available\n",
    "        if None not in start_coord and None not in end_coord:\n",
    "            ax.plot(\n",
    "                [-start_coord[2], -end_coord[2]],  # x-coordinates\n",
    "                [start_coord[0], end_coord[0]],    # y-coordinates\n",
    "                [start_coord[1], end_coord[1]],    # z-coordinates\n",
    "                color='blue'\n",
    "            )\n",
    "\n",
    "    # Set consistent axis limits\n",
    "    ax.set_xlim([-z_max, -z_min])\n",
    "    ax.set_ylim([x_min, x_max])\n",
    "    ax.set_zlim([y_min, y_max])\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Z')\n",
    "    ax.set_ylabel('X')\n",
    "    ax.set_zlabel('Y')\n",
    "\n",
    "    # Flip the Z-axis to ensure head is at the top and legs at the bottom\n",
    "    ax.invert_zaxis()\n",
    "\n",
    "    # Maintain aspect ratio\n",
    "    ax.set_box_aspect([1, 1, 1])  # Same scaling for all axes\n",
    "\n",
    "    # Rotate, tilt, and twist the graph\n",
    "    ax.view_init(elev=10, azim=20)  # Adjust 'elev' and 'azim' as needed\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title(f'3D Pose for Frame {frame_number}')\n",
    "\n",
    "    # Save the plot as an image\n",
    "    filename = f'frames/frame_{frame_number:03d}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "# Create a video from the saved images\n",
    "with imageio.get_writer('pose_video_2.mp4', fps=10) as writer:\n",
    "    for frame_number in range(df_trail_run['frame'].max() + 1):\n",
    "        filename = f'frames/frame_{frame_number:03d}.png'\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(\"Video saved as 'pose_video_2.mp4'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Mediapipe Landmark Features per Excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_feat = True\n",
    "feat_not_to_add_lst = [\n",
    "    \"barbell biceps curl\",\n",
    "    \"bench press\",\n",
    "    \"deadlift\",\n",
    "    \"lat pulldown\",\n",
    "    \"pull up\",\n",
    "    \"push-up\",\n",
    "    \"squat\",\n",
    "    \"t bar row\",\n",
    "    \"tricep dips\",\n",
    "    \"tricep pushdown\",\n",
    "    \"chest fly machine\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose and Drawing modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directory containing videos\n",
    "video_folder = 'excercise_videos'  # Update this with the path to your videos folder\n",
    "\n",
    "# Process all videos in the directory\n",
    "for excercise_type in os.listdir(video_folder):\n",
    "    if add_new_feat:\n",
    "        if excercise_type not in feat_not_to_add_lst:\n",
    "            print(excercise_type)\n",
    "            csv_data = create_features_per_excercise(video_folder, excercise_type,mp_pose)\n",
    "            feat_to_parquet(csv_data, excercise_type)\n",
    "            print(excercise_type)\n",
    "            \n",
    "print(\"All videos processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_lst = [\n",
    "    'decline bench press_features.parquet',\n",
    "    'incline bench press_features.parquet'\n",
    "]\n",
    "\n",
    "df_excercise = pd.DataFrame()\n",
    "data_dir = r'\\data'\n",
    "for prqt in tqdm(os.listdir(data_dir)):\n",
    "    if prqt not in exclude_lst:\n",
    "        prqt_path = data_dir + '\\\\' + prqt\n",
    "        df_temp = pd.read_parquet(prqt_path)\n",
    "        df_excercise = pd.concat((df_excercise,df_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excercise = pd.read_parquet(r'\\pose_detection\\video_features_20.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = df_excercise.copy()\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial Run --> One Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = 1\n",
    "frame_num = 10\n",
    "\n",
    "df_trial = df_videos[(df_videos['exc_name'] == 'bench press') & \n",
    "    (df_videos['vid_num'] == vid_num) & \n",
    "    (df_videos['frame'] == frame_num)\n",
    "    ]\n",
    "df_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and store coordinates for all landmarks in a dictionary\n",
    "coordinates = {landmark: get_coords(landmark, df_trial) for landmark in LANDMARKS}\n",
    "body_vecs = get_body_vecs(coordinates)\n",
    "body_angles = calculate_body_angles(body_vecs)\n",
    "body_heights = calculate_body_heights(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape: Group by 'vid_num' to create sequences (each sequence = one video)\n",
    "label_encoder = LabelEncoder()\n",
    "df_videos['exc_name_encoded'] = label_encoder.fit_transform(df_videos['exc_name'])\n",
    "xyz_feat = [f'{axis}_{i}' for i in range(33) for axis in ['x', 'y', 'z']]\n",
    "\n",
    "# Parameters\n",
    "NUM_LANDMARKS = 33  \n",
    "FEATURES_PER_LANDMARK = 3  \n",
    "FRAME_FEATURES = NUM_LANDMARKS * FEATURES_PER_LANDMARK\n",
    "\n",
    "# Ensure consistent landmark order across frames\n",
    "expected_landmarks = df_videos['landmark'].unique()\n",
    "\n",
    "# Group by video number (vid_num)\n",
    "grouped_videos = df_videos.groupby(['exc_name', 'vid_num'])\n",
    "\n",
    "# Initialize lists for videos and labels\n",
    "df_videos = pd.DataFrame()\n",
    "count = 0\n",
    "# Process each video\n",
    "for (exercise_name, vid_num), group in tqdm(grouped_videos):\n",
    "    try:\n",
    "        count += 1\n",
    "        # Sort by frame to ensure correct sequence\n",
    "        group = group.sort_values(by='frame')\n",
    "\n",
    "        # Store frames for this video\n",
    "        df_vid = pd.DataFrame()\n",
    "\n",
    "        # Group by frame number within each video\n",
    "        for frame_num, frame in group.groupby('frame'):\n",
    "            # Ensure all 33 landmarks are present; fill missing ones with zeros\n",
    "            frame = frame.set_index('landmark').reindex(expected_landmarks, fill_value=0)\n",
    "            # Flatten the landmarks for this frame (shape: (99,))\n",
    "            df_frame_x = pd.DataFrame([frame[['x', 'y', 'z']].values.flatten()], columns=xyz_feat)\n",
    "            # Get the body features\n",
    "            coordinates = {landmark: get_coords(landmark, frame.reset_index()) for landmark in LANDMARKS}\n",
    "            body_vecs = get_body_vecs(coordinates)\n",
    "            # Get the angles\n",
    "            body_angles = calculate_body_angles(body_vecs)\n",
    "            df_angle_frame_x = pd.DataFrame([body_angles],columns=BODY_ANGLES)\n",
    "            df_frame_x = pd.concat((df_frame_x,df_angle_frame_x), axis=1)\n",
    "            # Get the heights\n",
    "            body_heights = calculate_body_heights(coordinates)\n",
    "            df_height_frame_x = pd.DataFrame([body_heights],columns=BODY_HEIGHTS)\n",
    "            df_frame_x = pd.concat((df_frame_x,df_height_frame_x), axis=1)\n",
    "            \n",
    "            # Record the video number and frame number\n",
    "            df_frame_vid = pd.DataFrame({\n",
    "                'vid_num': [vid_num],\n",
    "                'frame_num': [frame_num]\n",
    "            })\n",
    "            df_frame_x = pd.concat((df_frame_x,df_frame_vid), axis=1)\n",
    "\n",
    "            # Append this frame's info to the video DataFrame\n",
    "            df_vid = pd.concat((df_vid, df_frame_x), axis=0)\n",
    "            \n",
    "            # Add the label as a final column\n",
    "            # Add the label column\n",
    "            df_frame_x['label'] = exercise_name\n",
    "\n",
    "            df_vid = pd.concat((df_vid,df_frame_x), axis = 0)\n",
    "        \n",
    "        df_videos = pd.concat((df_videos,df_vid), axis=0)\n",
    "        # if count == 2:\n",
    "        #     break\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(exercise_name, e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = pd.read_parquet('video_features_22.parquet')\n",
    "\n",
    "# Select Which Features to use\n",
    "df_videos = df_videos[~df_videos['label'].isin(['decline bench press', 'incline bench press','romanian deadlift'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_videos['exc_name_encoded'] = label_encoder.fit_transform(df_videos['label'])\n",
    "\n",
    "\n",
    "# Save the LabelEncoder as a pickle file\n",
    "with open('label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length\n",
    "desired_sequence_length = 30\n",
    "\n",
    "# Group by 'vid_num' and get the number of videos\n",
    "grouped_videos = df_videos.groupby(['label','vid_num'])\n",
    "\n",
    "# Initialize a list to store split data\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "for excercise_type, group_data_excercise in grouped_videos:\n",
    "    group_data_excercise = group_data_excercise.reset_index(drop=True)\n",
    "    \n",
    "    # Process in chunks of `desired_sequence_length`\n",
    "    for i in range(0, group_data_excercise.shape[0], desired_sequence_length):\n",
    "        # Select the first `desired_sequence_length` rows\n",
    "        sequence_chunk = group_data_excercise.iloc[i:i + desired_sequence_length, :]\n",
    "        if sequence_chunk.shape[0] == desired_sequence_length:\n",
    "            # Append the sequence to our list\n",
    "            all_sequences.append(sequence_chunk.drop(['label','exc_name_encoded','vid_num','frame_num'], axis=1))\n",
    "            all_labels.append(sequence_chunk['exc_name_encoded'].reset_index(drop=True)[0])\n",
    "        else:\n",
    "            padded_sequence = pad_sequence(sequence_chunk,desired_sequence_length )\n",
    "            all_sequences.append(padded_sequence.drop(['label','exc_name_encoded','vid_num','frame_num'], axis=1))\n",
    "            all_labels.append(padded_sequence['exc_name_encoded'].reset_index(drop=True)[0])\n",
    "\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80% train, 20% test)\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    all_sequences, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Train X Shape: {train_X.shape}, Train y Shape: {train_y.shape}\")\n",
    "print(f\"Test X Shape: {test_X.shape}, Test y Shape: {test_y.shape}\")\n",
    "\n",
    "# Ensure labels are in the correct dtype (int32)\n",
    "train_y = train_y.astype(np.int32)\n",
    "test_y = test_y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_classifier(sequence_len, input_dim, num_classes, \n",
    "                           d_model=64, num_heads=4, ff_dim=128, \n",
    "                           num_layers=2, dropout_rate=0.2):\n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=(sequence_len, input_dim))\n",
    "\n",
    "    # Project input features to d_model dimensions\n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "\n",
    "    # Positional Encoding with learnable embeddings\n",
    "    position_embedding = tf.keras.layers.Embedding(input_dim=sequence_len, output_dim=d_model)\n",
    "    positions = tf.range(start=0, limit=sequence_len, delta=1)\n",
    "    pos_encoding = position_embedding(positions)\n",
    "    x = x + pos_encoding  # Add positional encoding to input\n",
    "\n",
    "    # Transformer Encoder Layers\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-Head Attention\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(x, x)\n",
    "        attn_output = tf.keras.layers.Dropout(dropout_rate)(attn_output)\n",
    "        attn_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        # Feedforward Network\n",
    "        ff_output = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "        ])(attn_output)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn_output + ff_output)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dropout before final output\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer for classification\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_classes = len(np.unique(train_y))  # Number of exercise classes\n",
    "sequence_len = train_X.shape[1]  # Number of frames (max_frames)\n",
    "input_dim = train_X.shape[2]  # 99 features per frame\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3, decay_steps=10000, alpha=1e-5\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Build and compile the model\n",
    "transform_model = transformer_classifier(sequence_len, input_dim, num_classes)\n",
    "transform_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "transform_model.summary()\n",
    "\n",
    "# Flatten `all_labels` to ensure it's 1D\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "# Calculate class weights\n",
    "classes = np.unique(all_labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=all_labels)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint to save the best model weights based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='transformer_19.h5',   \n",
    "    monitor='val_accuracy',             \n",
    "    save_best_only=True,                \n",
    "    mode='max',                         \n",
    "    verbose=1                           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with class weights\n",
    "history = transform_model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    validation_data=(test_X, test_y),\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = transform_model.evaluate(test_X, test_y)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_model.load_weights('transformer_19.h5')\n",
    "transform_model.save('transformer_19_30_frames.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(transform_model.predict(test_X), axis=1)\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "display_labels = np.unique(test_y)\n",
    "# Assuming y_test contains the true labels and final_predictions are the ensemble predictions\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamater Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Automatically determine the number of unique classes\n",
    "    num_classes = len(np.unique(train_y))  # Make sure train_y is defined\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    d_model = hp.Int('d_model', min_value=64, max_value=256, step=64)\n",
    "    num_heads = hp.Choice('num_heads', values=[4, 8, 12])\n",
    "    ff_dim = hp.Int('ff_dim', min_value=128, max_value=512, step=128)\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=4, step=1)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-4, 5e-5, 1e-5])\n",
    "\n",
    "    # Build the Transformer model\n",
    "    inputs = tf.keras.Input(shape=(60, 99))  # Adjust based on your input shape\n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "\n",
    "    position_embedding = tf.keras.layers.Embedding(input_dim=60, output_dim=d_model)\n",
    "    positions = tf.range(start=0, limit=60, delta=1)\n",
    "    pos_encoding = position_embedding(positions)\n",
    "    pos_encoding = tf.keras.layers.Dense(d_model)(pos_encoding)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(x_norm, x_norm)\n",
    "        attn_output = tf.keras.layers.Dropout(dropout_rate)(attn_output)\n",
    "        x = x + attn_output\n",
    "\n",
    "        ff_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])(ff_norm)\n",
    "        x = x + ff_output\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer with correct number of classes\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,  \n",
    "    directory='my_tuning_dir',\n",
    "    project_name='transformer_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_X, tf.keras.utils.to_categorical(train_y),\n",
    "             validation_data=(test_X, tf.keras.utils.to_categorical(test_y)),\n",
    "             epochs=50, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best d_model: {best_hps.get('d_model')}\")\n",
    "print(f\"Best num_heads: {best_hps.get('num_heads')}\")\n",
    "print(f\"Best ff_dim: {best_hps.get('ff_dim')}\")\n",
    "print(f\"Best learning_rate: {best_hps.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model(best_hps):\n",
    "    num_classes = len(np.unique(train_y))\n",
    "    # Extract hyperparameters from the best_hps object\n",
    "    d_model = best_hps.get('d_model')\n",
    "    num_heads = best_hps.get('num_heads')\n",
    "    ff_dim = best_hps.get('ff_dim')\n",
    "    num_layers = best_hps.get('num_layers')  \n",
    "    dropout_rate = best_hps.get('dropout_rate')\n",
    "    learning_rate = best_hps.get('learning_rate')\n",
    "\n",
    "    # Define the Transformer model using the best hyperparameters\n",
    "    inputs = tf.keras.Input(shape=(60, 99))  \n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "\n",
    "    # Positional encoding\n",
    "    position_embedding = tf.keras.layers.Embedding(input_dim=60, output_dim=d_model)\n",
    "    positions = tf.range(start=0, limit=60, delta=1)\n",
    "    pos_encoding = position_embedding(positions)\n",
    "    pos_encoding = tf.keras.layers.Dense(d_model)(pos_encoding)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    # Transformer encoder layers\n",
    "    for _ in range(num_layers):\n",
    "        x_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(x_norm, x_norm)\n",
    "        attn_output = tf.keras.layers.Dropout(dropout_rate)(attn_output)\n",
    "        x = x + attn_output  # Residual connection\n",
    "\n",
    "        ff_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "        ])(ff_norm)\n",
    "        x = x + ff_output  \n",
    "\n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Final dropout and output layer\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model with the best learning rate\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best hyperparameters\n",
    "final_model = build_final_model(best_hps)\n",
    "\n",
    "# Print the model summary to verify the architecture\n",
    "final_model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = final_model.fit(\n",
    "    train_X, tf.keras.utils.to_categorical(train_y),\n",
    "    validation_data=(test_X, tf.keras.utils.to_categorical(test_y)),\n",
    "    epochs=200, batch_size=32,  \n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_final_model.h5', save_best_only=True)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(final_model.predict(test_X), axis=1)\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "display_labels = np.unique(test_y)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_model = tf.keras.models.load_model('transformer_19.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mediapipe features live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "# Parameters\n",
    "NUM_LANDMARKS = 33  \n",
    "FEATURES_PER_LANDMARK = 3  \n",
    "FRAME_FEATURES =  147 \n",
    "WINDOW_SIZE = 120 \n",
    "chosen_model = transform_model\n",
    "# Video source\n",
    "video_path = r'\\pose_detection\\Random Workout clips.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize a buffer for 60 frames\n",
    "buffer = []\n",
    "\n",
    "# Create a named window and set its size only once\n",
    "cv2.namedWindow(\"Live Prediction\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Live Prediction\", 300, 300)  # Set the window to the desired size\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame with Mediapipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        # Extract landmarks and flatten to (99,)\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "        if landmarks.shape[0] == FRAME_FEATURES:\n",
    "            # Add the frame to the buffer\n",
    "            buffer.append(landmarks)\n",
    "\n",
    "            # If buffer exceeds 60 frames, remove the oldest frame\n",
    "            if len(buffer) > WINDOW_SIZE:\n",
    "                buffer.pop(0)\n",
    "\n",
    "            # Make a prediction when buffer has exactly 60 frames\n",
    "            if len(buffer) == WINDOW_SIZE:\n",
    "                input_data = np.expand_dims(np.array(buffer), axis=0)\n",
    "\n",
    "                # Uncomment after loading your model\n",
    "                prediction = chosen_model.predict(input_data)\n",
    "                print(prediction)\n",
    "                predicted_label = label_encoder.inverse_transform(np.argmax(prediction, axis=1))\n",
    "\n",
    "                # Simulate a prediction label for now\n",
    "                # predicted_label = \"Sample Exercise\"\n",
    "                \n",
    "                # Display the prediction on the frame\n",
    "                cv2.putText(frame, f'Exercise: {predicted_label}', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame in the correctly sized window\n",
    "    cv2.imshow(\"Live Prediction\", frame)\n",
    "\n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many Features live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "xyz_feat = [f'{axis}_{i}' for i in range(33) for axis in ['x', 'y', 'z']]\n",
    "\n",
    "# Parameters\n",
    "NUM_FEATURES =  147 \n",
    "WINDOW_SIZE = 60  # Adjust to your model input size\n",
    "chosen_model = transform_model\n",
    "# Video source\n",
    "video_path = r\"pose_detection\\doron_random_filter.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize a buffer for 60 frames\n",
    "buffer = []\n",
    "predicted_label = None\n",
    "\n",
    "# Create a named window and set its size only once\n",
    "cv2.namedWindow(\"Live Prediction\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Live Prediction\", 300, 300)  # Set the window to the desired size\n",
    "frame_feature_sequences = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break    \n",
    "\n",
    "    # Process the frame with Mediapipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "        coordinates = {}                      \n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            # print(f\"{mp_pose.PoseLandmark(idx).name}: (x: {landmark.x}, y: {landmark.y}, z: {landmark.z})\")\n",
    "            coordinates[mp_pose.PoseLandmark(idx).name] = [landmark.x, landmark.y, landmark.z]       \n",
    "\n",
    "        body_vecs = get_body_vecs(coordinates)\n",
    "        # Get the angles\n",
    "        body_angles = calculate_body_angles(body_vecs)\n",
    "        # Get the heights\n",
    "        body_heights = calculate_body_heights(coordinates)\n",
    "\n",
    "        frame_feature_sequence = list(landmarks) + list(body_angles.values()) + list(body_heights.values())\n",
    "\n",
    "        frame_feature_sequences.append(frame_feature_sequence)      \n",
    "        # print(len(frame_feature_sequences))\n",
    "        if len(frame_feature_sequences) == WINDOW_SIZE:\n",
    "                frame_feature_array = np.array(frame_feature_sequences)\n",
    "                frame_feature_array = np.expand_dims(frame_feature_array, axis=0)\n",
    "                # Uncomment after loading your model\n",
    "                prediction = chosen_model.predict(frame_feature_array)\n",
    "            \n",
    "                predicted_label = label_encoder.inverse_transform(np.argmax(prediction, axis=1))\n",
    "                print(predicted_label)\n",
    "                # Display the prediction on the frame\n",
    "                cv2.putText(frame, f'Exercise: {predicted_label}', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "                frame_feature_sequences = []\n",
    "        \n",
    "        else:\n",
    "            if predicted_label:\n",
    "                                  # Display the prediction on the frame\n",
    "                cv2.putText(frame, f'Exercise: {predicted_label}', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                                 # Display the prediction on the frame\n",
    "                cv2.putText(frame, 'detecting excercise', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame in the correctly sized window\n",
    "    cv2.imshow(\"Live Prediction\", frame)\n",
    "\n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many features process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "xyz_feat = [f'{axis}_{i}' for i in range(33) for axis in ['x', 'y', 'z']]\n",
    "\n",
    "# Parameters\n",
    "NUM_FEATURES = 147 \n",
    "WINDOW_SIZE = 60  # Adjust to your model input size\n",
    "chosen_model = transform_model\n",
    "# Video source\n",
    "video_path = r\"\\pose_detection\\doron_random_filter.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the total number of frames for progress calculation\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialize a buffer for 60 frames\n",
    "buffer = []\n",
    "predicted_label = None\n",
    "\n",
    "frame_feature_sequences = []\n",
    "\n",
    "# Video writer setup\n",
    "output_path = r'pose_detection\\process_doron_random_filter.mp4'\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "output_fps = original_fps * 2  # Save at double speed\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, output_fps, (frame_width, frame_height))\n",
    "\n",
    "# Track the current frame count\n",
    "current_frame = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break    \n",
    "\n",
    "    # Process the frame with Mediapipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "        coordinates = {}                      \n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            coordinates[mp_pose.PoseLandmark(idx).name] = [landmark.x, landmark.y, landmark.z]       \n",
    "\n",
    "        body_vecs = get_body_vecs(coordinates)\n",
    "        # Get the angles\n",
    "        body_angles = calculate_body_angles(body_vecs)\n",
    "        # Get the heights\n",
    "        body_heights = calculate_body_heights(coordinates)\n",
    "\n",
    "        frame_feature_sequence = list(landmarks) + list(body_angles.values()) + list(body_heights.values())\n",
    "        frame_feature_sequences.append(frame_feature_sequence)      \n",
    "\n",
    "        if len(frame_feature_sequences) == WINDOW_SIZE:\n",
    "                frame_feature_array = np.array(frame_feature_sequences)\n",
    "                frame_feature_array = np.expand_dims(frame_feature_array, axis=0)\n",
    "                # Uncomment after loading your model\n",
    "                prediction = chosen_model.predict(frame_feature_array)\n",
    "            \n",
    "                predicted_label = label_encoder.inverse_transform(np.argmax(prediction, axis=1))\n",
    "                \n",
    "                frame_feature_sequences = []\n",
    "        \n",
    "        # Overlay the prediction on the frame\n",
    "        if predicted_label:\n",
    "            cv2.putText(frame, f'Exercise: {predicted_label}', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(frame, 'Detecting exercise...', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Update progress\n",
    "    current_frame += 1\n",
    "    progress = (current_frame / total_frames) * 100\n",
    "    print(f\"Processing: {progress:.2f}% complete\", end='\\r')\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "pose.close()     \n",
    "out.release()\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "xyz_feat = [f'{axis}_{i}' for i in range(33) for axis in ['x', 'y', 'z']]\n",
    "\n",
    "# Parameters\n",
    "NUM_FEATURES = 147 \n",
    "WINDOW_SIZE = 60  # Adjust to your model input size\n",
    "chosen_model = transform_model\n",
    "video_path = r\"\\pose_detection\\doron_random_filter.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Limit the number of frames processed to 1000\n",
    "frame_limit = 1000\n",
    "\n",
    "# Initialize a buffer for 60 frames\n",
    "buffer = []\n",
    "predicted_label = None\n",
    "frame_feature_sequences = []\n",
    "\n",
    "# Video writer setup\n",
    "output_path = r'\\pose_detection\\process_doron_random_filter_1000.mp4'\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "output_fps = original_fps * 2  # Save at double speed\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, output_fps, (frame_width, frame_height))\n",
    "\n",
    "# Track the current frame count\n",
    "current_frame = 0\n",
    "\n",
    "while cap.isOpened() and current_frame < frame_limit:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break    \n",
    "\n",
    "    # Process the frame with Mediapipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "        coordinates = {}                      \n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            coordinates[mp_pose.PoseLandmark(idx).name] = [landmark.x, landmark.y, landmark.z]       \n",
    "\n",
    "        body_vecs = get_body_vecs(coordinates)\n",
    "        body_angles = calculate_body_angles(body_vecs)\n",
    "        body_heights = calculate_body_heights(coordinates)\n",
    "\n",
    "        frame_feature_sequence = list(landmarks) + list(body_angles.values()) + list(body_heights.values())\n",
    "        frame_feature_sequences.append(frame_feature_sequence)      \n",
    "\n",
    "        if len(frame_feature_sequences) == WINDOW_SIZE:\n",
    "                frame_feature_array = np.array(frame_feature_sequences)\n",
    "                frame_feature_array = np.expand_dims(frame_feature_array, axis=0)\n",
    "                # Uncomment after loading your model\n",
    "                prediction = chosen_model.predict(frame_feature_array)\n",
    "            \n",
    "                predicted_label = label_encoder.inverse_transform(np.argmax(prediction, axis=1))\n",
    "                \n",
    "                frame_feature_sequences = []\n",
    "        \n",
    "        # Overlay the prediction on the frame\n",
    "        if predicted_label:\n",
    "            cv2.putText(frame, f'Exercise: {predicted_label}', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(frame, 'Detecting exercise...', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Update progress\n",
    "    current_frame += 1\n",
    "    progress = (current_frame / frame_limit) * 100\n",
    "    print(f\"Processing: {progress:.2f}% complete\", end='\\r')\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "pose.close()     \n",
    "out.release()\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
